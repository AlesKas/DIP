{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_analytics = pd.read_csv('./data/Network_Analytics.csv')\n",
    "# series = pd.Series(network_analytics['OutboundUtilzation (%)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of train_set : 80.00%\n",
      "Proportion of valid_set : 15.00%\n",
      "Proportion of test_set : 5.01%\n"
     ]
    }
   ],
   "source": [
    "train_len = int(len(network_analytics) * 0.80)\n",
    "valid_len = int(len(network_analytics) * 0.15)\n",
    "\n",
    "train = network_analytics[:train_len]\n",
    "valid = network_analytics[train_len:(train_len + valid_len)]\n",
    "test = network_analytics[train_len + len(valid):]\n",
    "\n",
    "assert len(network_analytics) == (len(train) + len(valid) + len(test))\n",
    "\n",
    "print('Proportion of train_set : {:.2f}%'.format(len(train)/len(network_analytics) * 100))\n",
    "print('Proportion of valid_set : {:.2f}%'.format(len(valid)/len(network_analytics) * 100))\n",
    "print('Proportion of test_set : {:.2f}%'.format(len(test)/len(network_analytics) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20504\n"
     ]
    }
   ],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(sequence, n_steps):\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        \n",
    "        end_ix = i + n_steps\n",
    "        \n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(x, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "raw_seq = [10,20,30,40,50,60,70,80,90]\n",
    "n_steps = 3\n",
    "train_x,train_y = split_sequence(train['OutboundUtilzation (%)'].values,n_steps)\n",
    "valid_x,valid_y = split_sequence(valid['OutboundUtilzation (%)'].values,n_steps)\n",
    "test_x,test_y = split_sequence(test['OutboundUtilzation (%)'].values,n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkAnalyticsDataset(Dataset):\n",
    "    def __init__(self,feature,target):\n",
    "        self.feature = feature\n",
    "        self.target = target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        item = self.feature[idx]\n",
    "        label = self.target[idx]\n",
    "        \n",
    "        return item, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesCNN(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super(TimeseriesCNN,self).__init__()\n",
    "        self.conv1d = nn.Conv1d(3, 64, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc1 = nn.Linear(64, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('output/checkpoints/model.pt'):\n",
    "    model = TimeseriesCNN()\n",
    "    model.load_state_dict(torch.load('output/checkpoints/model.pt'))\n",
    "    model.to(device)\n",
    "else:\n",
    "    model = TimeseriesCNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = NetworkAnalyticsDataset(train_x.reshape(train_x.shape[0],train_x.shape[1], 1), train_y)\n",
    "valid = NetworkAnalyticsDataset(valid_x.reshape(valid_x.shape[0],valid_x.shape[1], 1), valid_y)\n",
    "test = NetworkAnalyticsDataset(test_x.reshape(test_x.shape[0],test_x.shape[1], 1), test_y)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=1, shuffle=False)\n",
    "valid_loader = DataLoader(valid, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader : DataLoader):  \n",
    "    running_loss = .0\n",
    "    model.train()\n",
    "    \n",
    "    for idx, (inputs,labels) in enumerate(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(inputs.float())\n",
    "        loss = criterion(preds,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "        \n",
    "    train_loss = running_loss/len(loader)\n",
    "    return train_loss.detach().cpu().numpy()\n",
    "    \n",
    "def valid(loader : DataLoader):\n",
    "    running_loss = .0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(inputs.float())\n",
    "            loss = criterion(preds,labels)\n",
    "            running_loss += loss\n",
    "            \n",
    "        valid_loss = running_loss/len(loader)\n",
    "        return valid_loss.detach().cpu().numpy()\n",
    "\n",
    "def test(loader : DataLoader):\n",
    "    running_loss = .0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(inputs.float())\n",
    "            loss = criterion(preds,labels)\n",
    "            running_loss += loss\n",
    "            \n",
    "        test_loss = running_loss/len(loader)\n",
    "        return test_loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_losses.append(train(train_loader))\n",
    "    valid_losses.append(valid(valid_loader))\n",
    "    gc.collect()\n",
    "    torch.save(model.state_dict(), \"output/checkpoints/model.pt\")\n",
    "\n",
    "test_losses.append(test(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
